data(iris)
View(iris)
clear
data(iris)
view(iris)
?View
View(iris)
view(iris)
data()
data(iris)
view(iris)
?View
view(iris, iris)
View(iris)
iris_missing<-iris
rm(iris_missing)
clr
cls
iris_missing<-iris
iris_missing[c(3,30,40),3]<-NA
installed.packages()
install.packages("modeest")
library(modeest)
?mlv()
PL_mfv<-mlv(iris_missing$Petal.Length, method = "mfv", na.rm = TRUE)
PL_mfv
str(PL_mfv)
PL_mfv$M
is.na(iris_missing$Petal.Length)
iris_missing[is.na(iris_missing$Petal.Length), "Petal.Length"]<-PL_mfv$M
# in the above section we are assigning our mode (most frequent value) to the missing parts of out dataset
iris_missing
# in the above section we are assigning our mode (most frequent value) to the missing parts of out dataset
View(iris_missing)
rm(*)
?rm
rm(iris, iris_missing, PL_mfv)
rm(list=ls())
data(iris)
View(iris)
length(iris)
nrow(iris)
iris_missing<-iris
iris_missing[c(3,30,40),3]
iris_missing[c(3,30,40),3]<-NA
summary( iris_missing)
?boxplot()
boxplot(iris_missing[1:3])
?hist()
hist(iris_missing$Sepal.Length)
?pairs()
pairs(iris[1:2] )
pairs(iris[1:4])
pairs(iris[1:4], main = "Iris Data ",      pch = 10 )
pairs(iris[1:4], main = "Anderson's Iris Data -- 3 species",
pch = 21, bg = c("red", "green3", "blue")[factor(iris$Species)])
?plot
plot(iris[,1:2])
iris_missing<-iris
iris_missing[c(3,30,40),3]<-NA
?na.omit
iris_missing2<-na.omit(iris_missing)
data(iris)
View(iris)
iris_missing<-iris
iris_missing[c(3,30,40),3]<-NA
installed.packages()
?install.packages
#install.packages("modeest")
library(modeest)
?mlv()
PL_mfv<-mlv(iris_missing$Petal.Length , method = "mfv",na.rm = TRUE)
str(PL_mfv)
PL_mfv$M
is.na(iris_missing$Petal.Length)
iris_missing[is.na(iris_missing$Petal.Length),"Petal.Length"]<-PL_mfv$M
iris_missing<-iris
iris_missing[c(3,30,40),3]<-NA
#install.packages('VIM')
library('VIM')
install.packages('VIM')
library('VIM')
library('VIM')
?kNN
iris_missing2<-kNN(iris_missing,variable ='Petal.Length',k=3,
dist_var=c('Sepal.Length','Sepal.Width'))
rm(list=ls())
mmnorm <-function(x,minx,maxx)
{z<-((x-minx)/(maxx-minx))
return(z)
}
mmnorm2 <-function(x)
{z<-((x-min(x))/(max(x)-min(x)))
return(z)
}
myvector<-1:20
mmnorm2(myvector)
myvector<-1:20
mmnorm(myvector,min(myvector),max(myvector))
maxx<-20
minx<-1
mmnorm4 <-function(x)
{
minx<-10
z<-((x-minx)/(maxx-minx))
y<-list(z,minx,maxx)
return(y)
}
results<-mmnorm4(myvector)
minx<-10
rm(minx,maxx)
mmnorm4 <-function(x)
{
minx<-10
maxx<-20
z<-((x-minx)/(maxx-minx))
y<-list(z,minx,maxx)
return(y)
}
results<-mmnorm4(myvector)
maxx<-40
minx<-10
mmnorm4 <-function(x)
{
minx<-10
z<-((x-minx)/(maxx-minx))
y<-list(z,minx,maxx)
return(y)
}
results<-mmnorm4(myvector)
results
maxx<-40
minx<-1
mmnorm4 <-function(x)
{
minx<-10
z<-((x-minx)/(maxx-minx))
y<-list(z,minx,maxx)
return(y)
}
results<-mmnorm4(myvector)
results
length(results)
results[1]
results[2]
results[3]
f_print <-function(x1,x2,x3,x4,x5)
{
print(c('value of x1',x1))
print(c('value of x2',x2))
print(c('value of x3',x3))
print(c('value of x4',x4))
print(c('value of x5',x5))
}
f_print(10,5,3,2,1)
f_print(x2=10,x5=5,3,2,1)
rm(list=ls())
#look at the list of packages
installed.packages()
library(class)
library()
library(class)
?knn()
data()
data("iris")
View(iris)
?sample()
range_1_100<- 1:100
sample(range_1_100, 80)
smpl80<-sort(sample(range_1_100))
smpl80<-sort(sample(range_1_100,80))
?sort()
smpl80
idx<-sort(sample(nrow(iris),as.integer(.65*nrow(iris))))
as.integer(.65*nrow(iris))
sample(nrow(iris),as.integer(.65*nrow(iris)))
sort(sample(nrow(iris),as.integer(.65*nrow(iris))))
idx<-sort(sample(nrow(iris),as.integer(.65*nrow(iris))))
idx
training<-iris[idx]
training<-iris[idx,]
test<-iris[-idx,]
library(class)
?knn()
predict<-knn(training[,-5], test[,-5], training[,5], k=3)
predict
table(Prediction=predict, Actual=test)
table(Prediction=predict, Actual=test[,5])
#########################################################
##  Step 0: Clear the environment
##
##
#########################################################
rm(list=ls())
#########################################################
##  Step 1: Load the relavent packages
##
##
#########################################################
installed.packages()
#install.packages("rpart")  # CART standard package
?install.packages()
install.packages("rpart")
install.packages("rpart.plot")     # Enhanced tree plots
install.packages("rattle")         # Fancy tree plot
install.packages("RColorBrewer")   # colors needed for rattle
library(rpart)
library(rpart.plot)  			# Enhanced tree plots
library(rattle)           # Fancy tree plot
library(RColorBrewer)     # colors needed for rattle
library(RColorBrewer)     # colors needed for rattle
install.packages("rattle")         # Fancy tree plot
library(rattle)           # Fancy tree plot
library(rattle)           # Fancy tree plot
library(RColorBrewer)     # colors needed for rattle
#########################################################
##  Step 0: Clear the environment
##
##
#########################################################
rm(list=ls())
clear
cls
slr
getwd()
#########################################################
# starting here is stuff we are writing in class
read.csv("C:/Users/Class2017/Desktop/OneDrive/Documents/CS 513/R/Titanic_rows.csv")
#########################################################
# starting here is stuff we are writing in class
data <- read.csv("C:/Users/Class2017/Desktop/OneDrive/Documents/CS 513/R/Titanic_rows.csv")
table(data$Survived)
?rpart()
library(rpart)
library(rpart.plot)  			# Enhanced tree plots
library(rattle)           # Fancy tree plot
library(RColorBrewer)     # colors needed for rattle
?rpart()
#grow the tree
mytree <- rpart( Survived~, data)
#grow the tree
mytree <- rpart( Survived, data)
#grow the tree
mytree <- rpart( Survived~,. data=data)
#grow the tree
mytree <- rpart( Survived~. data=data)
table(data$Survived)
#grow the tree
mytree <- rpart( Survived-, data=data)
#grow the tree
mytree <- rpart( Survived-. data=data)
#grow the tree
mytree <- rpart( Survived~,.data=data)
#grow the tree
mytree <- rpart( Survived-,.data=data)
#grow the tree
mytree <- rpart( Survived-.data=data)
#grow the tree
mytree <- rpart( Survived.data=data)
#grow the tree
mytree <- rpart(,Survived.data=data)
#grow the tree
mytree <- rpart(Survived.data=data)
#########################################################
# starting here is stuff we are writing in class
dsn <- read.csv("C:/Users/Class2017/Desktop/OneDrive/Documents/CS 513/R/Titanic_rows.csv")
#grow the tree
mytree <- rpart(Survived.data=dsn)
#grow the tree
mytree <- rpart( Survived.data=dsn)
# First step is to clear the environment
rm(list=ls())
getwd()
setwd("C:/Users/Class2017/Desktop/OneDrive/Documents/CS 513/R/project/machinelearning")
# Next we load our data
# Note that our working directory has been set to our lproject folder where this file, and the data are located
data <- read.csv(file = "recipeData.csv", na.strings=c("N/A"))
# Note that our KNN algorithm will not work with non-numeric values, so we can remove columns that we do not need
# We are using the same exact data set with our neural net as well, se we follow the same steps
data$SugarScale <- NULL
data$BrewMethod <- NULL
data$PrimingMethod <- NULL
data$PrimingAmount <- NULL
# Its a LOT of data so we are simply going to remove any row with NA's
data <- na.omit(data)
# Now we can separate the data into testing and training data
# We still have a lot of data, so we will be using a 50/50 split (taking every other observation)
every_other <- seq(1, nrow(data), by=2)
test_data <- data[every_other,5:18]
train_data <- data[-every_other,5:18]
#library("caret")
library("neuralnet")
all_vars <- colnames(data[5:18])
pred_vars <- all_vars[!all_vars%in%"StyleID"]
pred_vars <- paste(pred_vars, collapse = "+")
form= as.formula(paste("StyleID~", pred_vars, collapse = "+"))
test_data <- data[every_other,6:18]
train_data <- data[-every_other,6:18]
#library("caret")
library("neuralnet")
all_vars <- colnames(data[6:18])
pred_vars <- all_vars[!all_vars%in%"StyleID"]
View(data)
pred_vars <- paste(pred_vars, collapse = "+")
form= as.formula(paste("StyleID~", pred_vars, collapse = "+"))
net_bc21  <- neuralnet(formula = form, data=train_data, hidden=c(4,2), threshold=0.01)
#library("caret")
install.packages("neuralnet")
net_bc21  <- neuralnet(formula = form, data=train_data, hidden=c(4,2), threshold=0.01)
library("neuralnet")
net_bc21  <- neuralnet(formula = form, data=train_data, hidden=c(4,2), threshold=0.01)
net <- neuralnet(Style~Size.L.+OG+FG+ABV+IBU+Color+BoilSize+BoilTime+BoilGravity+Efficiency+MashThickness+PitchRate+PrimaryTemp, train_data, hidden = c(4,2), threshold = .01)
net <- neuralnet(Style~Size.L.+OG+FG+ABV+IBU+Color+BoilSize+BoilTime+BoilGravity+Efficiency+MashThickness+PitchRate+PrimaryTemp, train_data, hidden = c(4,2), threshold = .01)
# First step is to clear the environment
rm(list=ls())
# Next we load our data
# Note that our working directory has been set to our lproject folder where this file, and the data are located
data <- read.csv(file = "recipeData.csv", na.strings=c("N/A"))
# Note that our KNN algorithm will not work with non-numeric values, so we can remove columns that we do not need
# We are using the same exact data set with our neural net as well, se we follow the same steps
data$SugarScale <- NULL
data$BrewMethod <- NULL
data$PrimingMethod <- NULL
data$PrimingAmount <- NULL
# Its a LOT of data so we are simply going to remove any row with NA's
data <- na.omit(data)
# Now we can separate the data into testing and training data
# We still have a lot of data, so we will be using a 50/50 split (taking every other observation)
every_other <- seq(1, nrow(data), by=2)
test_data <- data[every_other,6:18]
train_data <- data[-every_other,6:18]
library("neuralnet")
net <- neuralnet(Style~Size.L.+OG+FG+ABV+IBU+Color+BoilSize+BoilTime+BoilGravity+Efficiency+MashThickness+PitchRate+PrimaryTemp, train_data, hidden = c(4,2), threshold = .01)
net <- neuralnet(StyleID~Size.L.+OG+FG+ABV+IBU+Color+BoilSize+BoilTime+BoilGravity+Efficiency+MashThickness+PitchRate+PrimaryTemp, train_data, hidden = c(4,2), threshold = .01)
test_data <- data[every_other,]
train_data <- data[-every_other,]
net <- neuralnet(StyleID~Size.L.+OG+FG+ABV+IBU+Color+BoilSize+BoilTime+BoilGravity+Efficiency+MashThickness+PitchRate+PrimaryTemp, train_data, hidden = c(4,2), threshold = .01)
# Run the neural net with
net <- neuralnet(StyleID~Size.L.+OG+FG+ABV+IBU+Color+BoilSize+BoilTime+BoilGravity+Efficiency+MashThickness+PitchRate+PrimaryTemp, train_data, hidden = c(4,2), threshold = .01)
# Run the neural net with
net <- neuralnet(StyleID~Size.L.+OG+FG+ABV+IBU+Color+BoilSize+BoilTime+BoilGravity+Efficiency+MashThickness+PitchRate+PrimaryTemp, train_data, hidden = c(4,2), threshold = .01)
plot(net)
net_results <- compute(net, test_data[,c(-1,-2,-3,-4,-5,-19)])
ANN=as.numeric(net_results$net.result)
ANN_round<-round(ANN)
wrong<- (test$StyleID!=ANN_round)
wrong<- (test_data$StyleID!=ANN_round)
rate<-sum(wrong)/length(wrong)
rate
net
# Run the neural net to predict the StyleID based on the other features
net <- neuralnet(StyleID~Size.L.+OG+FG+ABV+IBU+Color+BoilSize+BoilTime+BoilGravity+Efficiency+MashThickness+PitchRate+PrimaryTemp, train_data, hidden = 5, threshold = .01)
plot(net)
net_results <- compute(net, test_data[,c(-1,-2,-3,-4,-5,-19)])
ANN=as.numeric(net_results$net.result)
ANN_round<-round(ANN)
wrong<- (test_data$StyleID!=ANN_round)
rate<-sum(wrong)/length(wrong)
rate
# Run the neural net to predict the StyleID based on the other features
net <- neuralnet(StyleID~Size.L.+OG+FG+ABV+IBU+Color+BoilSize+BoilTime+BoilGravity+Efficiency+MashThickness+PitchRate+PrimaryTemp, train_data, hidden = 10, threshold = .01)
# Run the neural net to predict the StyleID based on the other features
net <- neuralnet(StyleID~Size.L.+OG+FG+ABV+IBU+Color+BoilSize+BoilTime+BoilGravity+Efficiency+MashThickness+PitchRate+PrimaryTemp, train_data, hidden = 10, threshold = .01)
# Run the neural net to predict the StyleID based on the other features
net <- neuralnet(StyleID~Size.L.+OG+FG+ABV+IBU+Color+BoilSize+BoilTime+BoilGravity+Efficiency+MashThickness+PitchRate+PrimaryTemp, train_data, hidden = c(6, 4, 2), threshold = .01)
plot(net)
net_results <- compute(net, test_data[,c(-1,-2,-3,-4,-5,-19)])
net_results
ANN=as.numeric(net_results$net.result)
ANN_round<-round(ANN)
wrong<- (test_data$StyleID!=ANN_round)
rate<-sum(wrong)/length(wrong)
rate
max(data$StyleID)
# Run the neural net to predict the StyleID based on the other features
net <- neuralnet(c(1:176)~Size.L.+OG+FG+ABV+IBU+Color+BoilSize+BoilTime+BoilGravity+Efficiency+MashThickness+PitchRate+PrimaryTemp, train_data, hidden = c(6, 4, 2), threshold = .01)
index<-sort(sample(nrow(dsn2),round(.25*nrow(dsn2))))
index<-sort(sample(nrow(data),round(.25*nrow(data))))
training<-dsn[-index,]
test<-dsn[index,]
training<-data[-index,]
test<-data[index,]
net <- neuralnet(StyleID~Size.L.+OG+FG+ABV+IBU+Color+BoilSize+BoilTime+BoilGravity+Efficiency+MashThickness+PitchRate+PrimaryTemp, training, hidden = c(6, 4, 2), threshold = .01)
plot(net)
net_results <- compute(net, test_data[,c(-1,-2,-3,-4,-5,-19)])
net_results <- compute(net, test[,c(-1,-2,-3,-4,-5,-19)])
ANN=as.numeric(net_results$net.result)
ANN_round<-round(ANN)
wrong<- (test_data$StyleID!=ANN_round)
rate<-sum(wrong)/length(wrong)
net_results <- compute(net, train[,c(-1,-2,-3,-4,-5,-19)])
net_results <- compute(net, trainign[,c(-1,-2,-3,-4,-5,-19)])
net_results <- compute(net, training[,c(-1,-2,-3,-4,-5,-19)])
ANN=as.numeric(net_results$net.result)
ANN_round<-round(ANN)
wrong<- (test_data$StyleID!=ANN_round)
rate<-sum(wrong)/length(wrong)
wrong<- (test$StyleID!=ANN_round)
rate<-sum(wrong)/length(wrong)
rate
?randomForest()
install.packages('randomForest')
library(randomForest)
?randomForest()
fit <- randomForest( Style~., data=train_data, importance=TRUE, ntree=1000)
dist<-dist(data[,-c(1,2,3,4,5,19)])
hclust_results<-hclust(dist)
dist<-dist(data[,-c(1,2,3,4,5,19)])
hclust_results<-hclust(dist)
hclust_results<-hclust(dist)
hclust_2<-cutree(hclust_results,2)
table(hclust_2,data[4])
table(hclust_2,data[,4])
kmeans_2<- kmeans(data[,-c(1,2,3,4,5,19)],2,nstart = 10)
kmeans_2$cluster
table(kmeans_2$cluster,bc2[,11])
table(kmeans_2$cluster,data[,4])
View(data)
table(hclust_results,data[,4])
table(as.atomic(hclust_results),data[,4])
# First step is to clear the environment
rm(list=ls())
# Next we load our data
# Note that our working directory has been set to our lproject folder where this file, and the data are located
data <- read.csv(file = "recipeData.csv", na.strings=c("N/A"))
# Note that our KNN algorithm will not work with non-numeric values, so we can remove columns that we do not need
# We are using the same exact data set with our neural net as well, se we follow the same steps
data$SugarScale <- NULL
data$BrewMethod <- NULL
data$PrimingMethod <- NULL
data$PrimingAmount <- NULL
# Its a LOT of data so we are simply going to remove any row with NA's
data <- na.omit(data)
# Now we can separate the data into testing and training data
# We still have a lot of data, so we will be using a 50/50 split (taking every other observation)
every_other <- seq(1, nrow(data), by=2)
test_data <- data[every_other,]
train_data <- data[-every_other,]
# c50 classification
C50_class <- C5.0( as.factor(Style)~.,data=train_data )
library("c50")
library('C50')
# c50 classification
C50_class <- C5.0( as.factor(Style)~.,data=train_data )
C50_class
# c50 classification
C50_class <- C5.0( as.factor(Style)~.,data=train_data[,c(-1,-2,-3,-4,-5,-19)] )
# c50 classification
C50_class <- C5.0( as.factor(Style)~.,data=train_data[,c(-1,-2,-3,-5,-19)] )
C50_class
# general information about the tree
summary(C50_class )
dev.off()
plot(C50_class)
# check error rate
C50_predict<-predict( C50_class ,test , type="Style" )
# check error rate
C50_predict<-predict( C50_class ,test , type="Style" )
# check error rate
C50_predict<-predict( C50_class ,test , type="class" )
# check error rate
C50_predict<-predict( C50_class ,test_data , type="class" )
table(actual=test_data[,4],C50=C50_predict)
wrong<- (test_data[,4]!=C50_predict)
c50_rate<-sum(wrong)/length(test_data[,4])
c50_rate
fit <- randomForest( Style~., data=train_data, importance=TRUE, ntree=1000)
View(data)
View(data)
