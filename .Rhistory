print(c('value of x1',x1))
print(c('value of x2',x2))
print(c('value of x3',x3))
print(c('value of x4',x4))
print(c('value of x5',x5))
}
f_print(10,5,3,2,1)
f_print(x2=10,x5=5,3,2,1)
rm(list=ls())
#look at the list of packages
installed.packages()
library(class)
library()
library(class)
?knn()
data()
data("iris")
View(iris)
?sample()
range_1_100<- 1:100
sample(range_1_100, 80)
smpl80<-sort(sample(range_1_100))
smpl80<-sort(sample(range_1_100,80))
?sort()
smpl80
idx<-sort(sample(nrow(iris),as.integer(.65*nrow(iris))))
as.integer(.65*nrow(iris))
sample(nrow(iris),as.integer(.65*nrow(iris)))
sort(sample(nrow(iris),as.integer(.65*nrow(iris))))
idx<-sort(sample(nrow(iris),as.integer(.65*nrow(iris))))
idx
training<-iris[idx]
training<-iris[idx,]
test<-iris[-idx,]
library(class)
?knn()
predict<-knn(training[,-5], test[,-5], training[,5], k=3)
predict
table(Prediction=predict, Actual=test)
table(Prediction=predict, Actual=test[,5])
#########################################################
##  Step 0: Clear the environment
##
##
#########################################################
rm(list=ls())
#########################################################
##  Step 1: Load the relavent packages
##
##
#########################################################
installed.packages()
#install.packages("rpart")  # CART standard package
?install.packages()
install.packages("rpart")
install.packages("rpart.plot")     # Enhanced tree plots
install.packages("rattle")         # Fancy tree plot
install.packages("RColorBrewer")   # colors needed for rattle
library(rpart)
library(rpart.plot)  			# Enhanced tree plots
library(rattle)           # Fancy tree plot
library(RColorBrewer)     # colors needed for rattle
library(RColorBrewer)     # colors needed for rattle
install.packages("rattle")         # Fancy tree plot
library(rattle)           # Fancy tree plot
library(rattle)           # Fancy tree plot
library(RColorBrewer)     # colors needed for rattle
#########################################################
##  Step 0: Clear the environment
##
##
#########################################################
rm(list=ls())
clear
cls
slr
getwd()
#########################################################
# starting here is stuff we are writing in class
read.csv("C:/Users/Class2017/Desktop/OneDrive/Documents/CS 513/R/Titanic_rows.csv")
#########################################################
# starting here is stuff we are writing in class
data <- read.csv("C:/Users/Class2017/Desktop/OneDrive/Documents/CS 513/R/Titanic_rows.csv")
table(data$Survived)
?rpart()
library(rpart)
library(rpart.plot)  			# Enhanced tree plots
library(rattle)           # Fancy tree plot
library(RColorBrewer)     # colors needed for rattle
?rpart()
#grow the tree
mytree <- rpart( Survived~, data)
#grow the tree
mytree <- rpart( Survived, data)
#grow the tree
mytree <- rpart( Survived~,. data=data)
#grow the tree
mytree <- rpart( Survived~. data=data)
table(data$Survived)
#grow the tree
mytree <- rpart( Survived-, data=data)
#grow the tree
mytree <- rpart( Survived-. data=data)
#grow the tree
mytree <- rpart( Survived~,.data=data)
#grow the tree
mytree <- rpart( Survived-,.data=data)
#grow the tree
mytree <- rpart( Survived-.data=data)
#grow the tree
mytree <- rpart( Survived.data=data)
#grow the tree
mytree <- rpart(,Survived.data=data)
#grow the tree
mytree <- rpart(Survived.data=data)
#########################################################
# starting here is stuff we are writing in class
dsn <- read.csv("C:/Users/Class2017/Desktop/OneDrive/Documents/CS 513/R/Titanic_rows.csv")
#grow the tree
mytree <- rpart(Survived.data=dsn)
#grow the tree
mytree <- rpart( Survived.data=dsn)
# First step is to clear the environment
rm(list=ls())
getwd()
setwd("C:/Users/Class2017/Desktop/OneDrive/Documents/CS 513/R/project/machinelearning")
# Next we load our data
# Note that our working directory has been set to our lproject folder where this file, and the data are located
data <- read.csv(file = "recipeData.csv", na.strings=c("N/A"))
# Note that our KNN algorithm will not work with non-numeric values, so we can remove columns that we do not need
# We are using the same exact data set with our neural net as well, se we follow the same steps
data$SugarScale <- NULL
data$BrewMethod <- NULL
data$PrimingMethod <- NULL
data$PrimingAmount <- NULL
# Its a LOT of data so we are simply going to remove any row with NA's
data <- na.omit(data)
# Now we can separate the data into testing and training data
# We still have a lot of data, so we will be using a 50/50 split (taking every other observation)
every_other <- seq(1, nrow(data), by=2)
test_data <- data[every_other,5:18]
train_data <- data[-every_other,5:18]
#library("caret")
library("neuralnet")
all_vars <- colnames(data[5:18])
pred_vars <- all_vars[!all_vars%in%"StyleID"]
pred_vars <- paste(pred_vars, collapse = "+")
form= as.formula(paste("StyleID~", pred_vars, collapse = "+"))
test_data <- data[every_other,6:18]
train_data <- data[-every_other,6:18]
#library("caret")
library("neuralnet")
all_vars <- colnames(data[6:18])
pred_vars <- all_vars[!all_vars%in%"StyleID"]
View(data)
pred_vars <- paste(pred_vars, collapse = "+")
form= as.formula(paste("StyleID~", pred_vars, collapse = "+"))
net_bc21  <- neuralnet(formula = form, data=train_data, hidden=c(4,2), threshold=0.01)
#library("caret")
install.packages("neuralnet")
net_bc21  <- neuralnet(formula = form, data=train_data, hidden=c(4,2), threshold=0.01)
library("neuralnet")
net_bc21  <- neuralnet(formula = form, data=train_data, hidden=c(4,2), threshold=0.01)
net <- neuralnet(Style~Size.L.+OG+FG+ABV+IBU+Color+BoilSize+BoilTime+BoilGravity+Efficiency+MashThickness+PitchRate+PrimaryTemp, train_data, hidden = c(4,2), threshold = .01)
net <- neuralnet(Style~Size.L.+OG+FG+ABV+IBU+Color+BoilSize+BoilTime+BoilGravity+Efficiency+MashThickness+PitchRate+PrimaryTemp, train_data, hidden = c(4,2), threshold = .01)
# First step is to clear the environment
rm(list=ls())
# Next we load our data
# Note that our working directory has been set to our lproject folder where this file, and the data are located
data <- read.csv(file = "recipeData.csv", na.strings=c("N/A"))
# Note that our KNN algorithm will not work with non-numeric values, so we can remove columns that we do not need
# We are using the same exact data set with our neural net as well, se we follow the same steps
data$SugarScale <- NULL
data$BrewMethod <- NULL
data$PrimingMethod <- NULL
data$PrimingAmount <- NULL
# Its a LOT of data so we are simply going to remove any row with NA's
data <- na.omit(data)
# Now we can separate the data into testing and training data
# We still have a lot of data, so we will be using a 50/50 split (taking every other observation)
every_other <- seq(1, nrow(data), by=2)
test_data <- data[every_other,6:18]
train_data <- data[-every_other,6:18]
library("neuralnet")
net <- neuralnet(Style~Size.L.+OG+FG+ABV+IBU+Color+BoilSize+BoilTime+BoilGravity+Efficiency+MashThickness+PitchRate+PrimaryTemp, train_data, hidden = c(4,2), threshold = .01)
net <- neuralnet(StyleID~Size.L.+OG+FG+ABV+IBU+Color+BoilSize+BoilTime+BoilGravity+Efficiency+MashThickness+PitchRate+PrimaryTemp, train_data, hidden = c(4,2), threshold = .01)
test_data <- data[every_other,]
train_data <- data[-every_other,]
net <- neuralnet(StyleID~Size.L.+OG+FG+ABV+IBU+Color+BoilSize+BoilTime+BoilGravity+Efficiency+MashThickness+PitchRate+PrimaryTemp, train_data, hidden = c(4,2), threshold = .01)
# Run the neural net with
net <- neuralnet(StyleID~Size.L.+OG+FG+ABV+IBU+Color+BoilSize+BoilTime+BoilGravity+Efficiency+MashThickness+PitchRate+PrimaryTemp, train_data, hidden = c(4,2), threshold = .01)
# Run the neural net with
net <- neuralnet(StyleID~Size.L.+OG+FG+ABV+IBU+Color+BoilSize+BoilTime+BoilGravity+Efficiency+MashThickness+PitchRate+PrimaryTemp, train_data, hidden = c(4,2), threshold = .01)
plot(net)
net_results <- compute(net, test_data[,c(-1,-2,-3,-4,-5,-19)])
ANN=as.numeric(net_results$net.result)
ANN_round<-round(ANN)
wrong<- (test$StyleID!=ANN_round)
wrong<- (test_data$StyleID!=ANN_round)
rate<-sum(wrong)/length(wrong)
rate
net
# Run the neural net to predict the StyleID based on the other features
net <- neuralnet(StyleID~Size.L.+OG+FG+ABV+IBU+Color+BoilSize+BoilTime+BoilGravity+Efficiency+MashThickness+PitchRate+PrimaryTemp, train_data, hidden = 5, threshold = .01)
plot(net)
net_results <- compute(net, test_data[,c(-1,-2,-3,-4,-5,-19)])
ANN=as.numeric(net_results$net.result)
ANN_round<-round(ANN)
wrong<- (test_data$StyleID!=ANN_round)
rate<-sum(wrong)/length(wrong)
rate
# Run the neural net to predict the StyleID based on the other features
net <- neuralnet(StyleID~Size.L.+OG+FG+ABV+IBU+Color+BoilSize+BoilTime+BoilGravity+Efficiency+MashThickness+PitchRate+PrimaryTemp, train_data, hidden = 10, threshold = .01)
# Run the neural net to predict the StyleID based on the other features
net <- neuralnet(StyleID~Size.L.+OG+FG+ABV+IBU+Color+BoilSize+BoilTime+BoilGravity+Efficiency+MashThickness+PitchRate+PrimaryTemp, train_data, hidden = 10, threshold = .01)
# Run the neural net to predict the StyleID based on the other features
net <- neuralnet(StyleID~Size.L.+OG+FG+ABV+IBU+Color+BoilSize+BoilTime+BoilGravity+Efficiency+MashThickness+PitchRate+PrimaryTemp, train_data, hidden = c(6, 4, 2), threshold = .01)
plot(net)
net_results <- compute(net, test_data[,c(-1,-2,-3,-4,-5,-19)])
net_results
ANN=as.numeric(net_results$net.result)
ANN_round<-round(ANN)
wrong<- (test_data$StyleID!=ANN_round)
rate<-sum(wrong)/length(wrong)
rate
max(data$StyleID)
# Run the neural net to predict the StyleID based on the other features
net <- neuralnet(c(1:176)~Size.L.+OG+FG+ABV+IBU+Color+BoilSize+BoilTime+BoilGravity+Efficiency+MashThickness+PitchRate+PrimaryTemp, train_data, hidden = c(6, 4, 2), threshold = .01)
index<-sort(sample(nrow(dsn2),round(.25*nrow(dsn2))))
index<-sort(sample(nrow(data),round(.25*nrow(data))))
training<-dsn[-index,]
test<-dsn[index,]
training<-data[-index,]
test<-data[index,]
net <- neuralnet(StyleID~Size.L.+OG+FG+ABV+IBU+Color+BoilSize+BoilTime+BoilGravity+Efficiency+MashThickness+PitchRate+PrimaryTemp, training, hidden = c(6, 4, 2), threshold = .01)
plot(net)
net_results <- compute(net, test_data[,c(-1,-2,-3,-4,-5,-19)])
net_results <- compute(net, test[,c(-1,-2,-3,-4,-5,-19)])
ANN=as.numeric(net_results$net.result)
ANN_round<-round(ANN)
wrong<- (test_data$StyleID!=ANN_round)
rate<-sum(wrong)/length(wrong)
net_results <- compute(net, train[,c(-1,-2,-3,-4,-5,-19)])
net_results <- compute(net, trainign[,c(-1,-2,-3,-4,-5,-19)])
net_results <- compute(net, training[,c(-1,-2,-3,-4,-5,-19)])
ANN=as.numeric(net_results$net.result)
ANN_round<-round(ANN)
wrong<- (test_data$StyleID!=ANN_round)
rate<-sum(wrong)/length(wrong)
wrong<- (test$StyleID!=ANN_round)
rate<-sum(wrong)/length(wrong)
rate
?randomForest()
install.packages('randomForest')
library(randomForest)
?randomForest()
fit <- randomForest( Style~., data=train_data, importance=TRUE, ntree=1000)
dist<-dist(data[,-c(1,2,3,4,5,19)])
hclust_results<-hclust(dist)
dist<-dist(data[,-c(1,2,3,4,5,19)])
hclust_results<-hclust(dist)
hclust_results<-hclust(dist)
hclust_2<-cutree(hclust_results,2)
table(hclust_2,data[4])
table(hclust_2,data[,4])
kmeans_2<- kmeans(data[,-c(1,2,3,4,5,19)],2,nstart = 10)
kmeans_2$cluster
table(kmeans_2$cluster,bc2[,11])
table(kmeans_2$cluster,data[,4])
View(data)
table(hclust_results,data[,4])
table(as.atomic(hclust_results),data[,4])
# First step is to clear the environment
rm(list=ls())
# Next we load our data
# Note that our working directory has been set to our lproject folder where this file, and the data are located
data <- read.csv(file = "recipeData.csv", na.strings=c("N/A"))
# Note that our KNN algorithm will not work with non-numeric values, so we can remove columns that we do not need
# We are using the same exact data set with our neural net as well, se we follow the same steps
data$SugarScale <- NULL
data$BrewMethod <- NULL
data$PrimingMethod <- NULL
data$PrimingAmount <- NULL
# Its a LOT of data so we are simply going to remove any row with NA's
data <- na.omit(data)
# Now we can separate the data into testing and training data
# We still have a lot of data, so we will be using a 50/50 split (taking every other observation)
every_other <- seq(1, nrow(data), by=2)
test_data <- data[every_other,]
train_data <- data[-every_other,]
# c50 classification
C50_class <- C5.0( as.factor(Style)~.,data=train_data )
library("c50")
library('C50')
# c50 classification
C50_class <- C5.0( as.factor(Style)~.,data=train_data )
C50_class
# c50 classification
C50_class <- C5.0( as.factor(Style)~.,data=train_data[,c(-1,-2,-3,-4,-5,-19)] )
# c50 classification
C50_class <- C5.0( as.factor(Style)~.,data=train_data[,c(-1,-2,-3,-5,-19)] )
C50_class
# general information about the tree
summary(C50_class )
dev.off()
plot(C50_class)
# check error rate
C50_predict<-predict( C50_class ,test , type="Style" )
# check error rate
C50_predict<-predict( C50_class ,test , type="Style" )
# check error rate
C50_predict<-predict( C50_class ,test , type="class" )
# check error rate
C50_predict<-predict( C50_class ,test_data , type="class" )
table(actual=test_data[,4],C50=C50_predict)
wrong<- (test_data[,4]!=C50_predict)
c50_rate<-sum(wrong)/length(test_data[,4])
c50_rate
fit <- randomForest( Style~., data=train_data, importance=TRUE, ntree=1000)
View(data)
View(data)
getwd()
# First step is to clear the environment
rm(list=ls())
# Next we load our data
# Note that our working directory has been set to our lproject folder where this file, and the data are located
data <- read.csv(file = "recipeData.csv", na.strings=c("N/A"))
# Note that our KNN algorithm will not work with non-numeric values, so we can remove columns that we do not need
data$SugarScale <- NULL
data$BrewMethod <- NULL
data$PrimingMethod <- NULL
data$PrimingAmount <- NULL
# Its a LOT of data so we are simply going to remove any row with NA's
data <- na.omit(data)
# Now we can separate the data into testing and training data
# We still have a lot of data, so we will be using a 50/50 split (taking every other observation)
every_other <- seq(1, nrow(data), by=2)
test_data <- data[every_other,]
train_data <- data[-every_other,]
# Load library
library(class)
# Run knn algorithm to predict the Style column
# When we run it we are not taking the BeerID, Name, URL, Style, StyleID, or UserID into account
# because they have nothing to do with the actual beer
knn5 <- knn(train_data[,c(-1,-2,-3,-4,-5,-19)], test_data[,c(-1,-2,-3,-4,-5,-19)], train_data[,4], k = 5)
# Check our accuracy
wrong <- (test_data[,4] != knn5)
rate <-sum(wrong)/length(wrong)
rate
# Now we can look at more values for K by editing this for loop
for(i in seq(1, 100, by=1)){
predict <- knn(train_data[,c(-1,-2,-3,-4,-5,-19)], test_data[,c(-1,-2,-3,-4,-5,-19)], train_data[,4], k = i)
wrong <- (test_data[,4]!=predict)
rate <- sum(wrong)/length(wrong)
#print(i)
#print(rate)
# Here we are storing our accuracy rates into a vector so that we can plot it and find which values are good for K
rates[i] <- rate * 100
}
rates <- []
rates <- c()
# Now we can look at more values for K by editing this for loop
for(i in seq(1, 100, by=1)){
predict <- knn(train_data[,c(-1,-2,-3,-4,-5,-19)], test_data[,c(-1,-2,-3,-4,-5,-19)], train_data[,4], k = i)
wrong <- (test_data[,4]!=predict)
rate <- sum(wrong)/length(wrong)
#print(i)
#print(rate)
# Here we are storing our accuracy rates into a vector so that we can plot it and find which values are good for K
rates[i] <- rate * 100
}
max(rates)
plot(rates)
plot(rates, xlab = "K Values", ylab = "% Accuracy", main = "KNN")
# First step is to clear the environment
rm(list=ls())
# Next we load our data
# Note that our working directory has been set to our lproject folder where this file, and the data are located
data <- read.csv(file = "recipeData.csv", na.strings=c("N/A"))
# Note that our KNN algorithm will not work with non-numeric values, so we can remove columns that we do not need
# We must eliminate more columns this time to appease the random forest algorithm
#(we can't have a column with factors of more than 53 categories)
data$SugarScale <- NULL
data$BrewMethod <- NULL
data$PrimingMethod <- NULL
data$PrimingAmount <- NULL
# Its a LOT of data so we are simply going to remove any row with NA's
data <- na.omit(data)
# these are the extra removals
data$Name <- NULL
data$URL <- NULL
data$Style <- NULL
data$UserId <- NULL
# install and load packages
#install.packages('randomForest')
#install.packages("dplyr")
library(dplyr)
library(randomForest)
# separate data into bins so we can test with the random forest
bin1 <- filter(data, StyleID<=50)
bin2 <- filter(data, StyleID>50 & StyleID<=100)
bin3 <- filter(data, StyleID>100 & StyleID<=150)
bin4 <- filter(data, StyleID>150)
# check to make sure that we filtered properly, sum should equal 7053
sum(nrow(bin1),nrow(bin2),nrow(bin3),nrow(bin4))
# set seed for random forest
set.seed(456)
# get testing and training data for each bin
everyother1 <- seq(1, nrow(bin1), by=2)
testbin1 <- bin1[everyother1,]
trainbin1 <- bin1[-everyother1,]
everyother2 <- seq(1, nrow(bin2), by=2)
testbin2 <- bin2[everyother2,]
trainbin2 <- bin2[-everyother2,]
everyother3 <- seq(1, nrow(bin3), by=2)
testbin3 <- bin3[everyother3,]
trainbin3 <- bin3[-everyother3]
everyother4 <- seq(1, nrow(bin4), by=2)
testbin4 <- bin4[everyother4,]
trainbin4 <- bin4[-everyother4,]
##################################################################################################
# bin 1
fit_bin1 <- randomForest(StyleID~., data = trainbin1[], importance=TRUE, ntree=1000)
importance(fit_bin1)
varImpPlot(fit_bin1)
prediction_bin1 <- predict(fit_bin1,testbin1)
# because we are predicting ID's we must round our predictions
prediction_bin1 <- round(prediction_bin1)
#test accuracy of the algorithm
wrong_bin1 <- (testbin1[,2]!=prediction_bin1)
errorrate_bin1 <- sum(wrong_bin1)/length(wrong_bin1)
errorrate_bin1
##################################################################################################
# bin 2
fit_bin2 <- randomForest(StyleID~., data = trainbin2[], importance=TRUE, ntree=1000)
importance(fit_bin2)
varImpPlot(fit_bin2)
prediction_bin2 <- predict(fit_bin2,testbin2)
# because we are predicting ID's we must round our predictions
prediction_bin2 <- round(prediction_bin2)
#test accuracy of the algorithm
wrong_bin2 <- (testbin2[,2]!=prediction_bin2)
errorrate_bin2 <- sum(wrong_bin2)/length(wrong_bin2)
errorrate_bin2
##################################################################################################
# bin 3
fit_bin3 <- randomForest(StyleID~., data = trainbin3[], importance=TRUE, ntree=1000)
importance(fit_bin3)
varImpPlot(fit_bin3)
prediction_bin3 <- predict(fit_bin3,testbin3)
# because we are predicting ID's we must round our predictions
prediction_bin3 <- round(prediction_bin3)
#test accuracy of the algorithm
wrong_bin3 <- (testbin3[,2]!=prediction_bin3)
errorrate_bin3 <- sum(wrong_bin3)/length(wrong_bin3)
errorrate_bin3
##################################################################################################
# bin 4
fit_bin4 <- randomForest(StyleID~., data = trainbin4[], importance=TRUE, ntree=1000)
importance(fit_bin4)
varImpPlot(fit_bin4)
prediction_bin4 <- predict(fit_bin4,testbin4)
# because we are predicting ID's we must round our predictions
prediction_bin4 <- round(prediction_bin4)
#test accuracy of the algorithm
wrong_bin4 <- (testbin4[,2]!=prediction_bin4)
errorrate_bin4 <- sum(wrong_bin4)/length(wrong_bin4)
errorrate_bin4
every_other <- seq(1, nrow(data), by=2)
test_data <- data[every_other,]
train_data <- data[-every_other,]
fit_data <- randomForest(StyleID~., data = train_data[], importance=TRUE, ntree=1000)
importance(fit_data)
varImpPlot(fit_data)
prediction_data <- predict(fit_data, test_data)
prediction_data <- round(prediction_data)
wrong_data <- (test_data[,2]!=prediction_data)
errorrate_data <- sum(wrong_data)/length(wrong_data)
errorrate_data
varImpPlot(fit_data, main = "Full Data Variable Importance")
# First step is to clear the environment
rm(list=ls())
# Next we load our data
# Note that our working directory has been set to our lproject folder where this file, and the data are located
data <- read.csv(file = "recipeData.csv", na.strings=c("N/A"))
# Note that our KNN algorithm will not work with non-numeric values, so we can remove columns that we do not need
# We are using the same exact data set with our neural net as well, se we follow the same steps
data$SugarScale <- NULL
data$BrewMethod <- NULL
data$PrimingMethod <- NULL
data$PrimingAmount <- NULL
# Its a LOT of data so we are simply going to remove any row with NA's
data <- na.omit(data)
# Now we can separate the data into testing and training data
# We still have a lot of data, so we will be using a 50/50 split (taking every other observation)
every_other <- seq(1, nrow(data), by=2)
test_data <- data[every_other,]
train_data <- data[-every_other,]
#install.packages('c50')
library('C50')
# c50 classification
C50_class <- C5.0( as.factor(Style)~.,data=train_data[,c(-1,-2,-3,-5,-19)] )
C50_class
# general information about the tree
summary(C50_class)
dev.off()
# check error rate, coincides with the returned error rate of the c5.0 function call
C50_predict<-predict( C50_class ,test_data , type="class" )
table(actual=test_data[,4],C50=C50_predict)
wrong<- (test_data[,4]!=C50_predict)
c50_rate<-sum(wrong)/length(test_data[,4])
c50_rate
